{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import os, sys\n",
    "import logging\n",
    "import random\n",
    "import h5py\n",
    "import shutil\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sigpy.plot as pl\n",
    "import torch\n",
    "import sigpy as sp\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "# import custom libraries\n",
    "from utils import transforms as T\n",
    "from utils import subsample as ss\n",
    "from utils import complex_utils as cplx\n",
    "from utils.resnet2p1d import generate_model\n",
    "from utils.flare_utils import roll\n",
    "# import custom classes\n",
    "from utils.datasets import SliceData\n",
    "from subsample_fastmri import MaskFunc\n",
    "from MoDL_single import UnrolledModel\n",
    "import argparse\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    \"\"\"\n",
    "    Data Transformer for training unrolled reconstruction models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_func, args, use_seed=False):\n",
    "        self.mask_func = mask_func\n",
    "        self.use_seed = use_seed\n",
    "        self.rng = np.random.RandomState()\n",
    "\n",
    "    def __call__(self, kspace, target, slice):\n",
    "        im_lowres = abs(sp.ifft(sp.resize(sp.resize(kspace,(640,24)),(640,372))))\n",
    "        magnitude_vals = im_lowres.reshape(-1)\n",
    "        k = int(round(0.05 * magnitude_vals.shape[0]))\n",
    "        scale = magnitude_vals[magnitude_vals.argsort()[::-1][k]]\n",
    "        kspace = kspace/scale\n",
    "        target = target/scale\n",
    "        # Convert everything from numpy arrays to tensors\n",
    "        kspace_torch = cplx.to_tensor(kspace).float()   \n",
    "        target_torch = cplx.to_tensor(target).float()   \n",
    "        mask_slice = np.ones((640,372))\n",
    "        mk1 = self.mask_func((1,1,372,2))[0,0,:,0]\n",
    "        knee_masks = mask_slice*mk1\n",
    "        mask_torch = torch.tensor(knee_masks[...,None]).float()   \n",
    "        kspace_torch = kspace_torch*mask_torch\n",
    "\n",
    "        return kspace_torch,target_torch,mask_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(args):\n",
    "    # Generate k-t undersampling masks\n",
    "    train_mask = MaskFunc([0.08],[4])\n",
    "    train_data = SliceData(\n",
    "        root=str(args.data_path),\n",
    "        transform=DataTransform(train_mask, args),\n",
    "        sample_rate=1\n",
    "    )\n",
    "    return train_data\n",
    "def create_data_loaders(args):\n",
    "    train_data = create_datasets(args)\n",
    "#     print(train_data[0])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader\n",
    "def build_optim(args, params):\n",
    "    optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "params = Namespace()\n",
    "params.data_path = \"train/\"\n",
    "params.batch_size = 2\n",
    "params.num_grad_steps = 4\n",
    "params.num_cg_steps = 8\n",
    "params.share_weights = True\n",
    "params.modl_lamda = 0.05\n",
    "params.lr = 0.0001\n",
    "params.weight_decay = 0\n",
    "params.lr_step_size = 500\n",
    "params.lr_gamma = 0.5\n",
    "params.epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_data_loaders(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared weights\n"
     ]
    }
   ],
   "source": [
    "single_MoDL = UnrolledModel(params).to(device)\n",
    "optimizer = build_optim(params, single_MoDL.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, params.lr_step_size, params.lr_gamma)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kewang/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "INFO:root:Epoch = [  0/200] Iter = [   0/ 125] Loss = 0.1586 Avg Loss = 0.1586\n",
      "INFO:root:Epoch = [  0/200] Iter = [  20/ 125] Loss = 0.1115 Avg Loss = 0.1515\n",
      "INFO:root:Epoch = [  0/200] Iter = [  40/ 125] Loss = 0.1002 Avg Loss = 0.1459\n",
      "INFO:root:Epoch = [  0/200] Iter = [  60/ 125] Loss = 0.145 Avg Loss = 0.1398\n",
      "INFO:root:Epoch = [  0/200] Iter = [  80/ 125] Loss = 0.1029 Avg Loss = 0.1359\n",
      "INFO:root:Epoch = [  0/200] Iter = [ 100/ 125] Loss = 0.05958 Avg Loss = 0.1298\n",
      "INFO:root:Epoch = [  0/200] Iter = [ 120/ 125] Loss = 0.1171 Avg Loss = 0.1279\n",
      "INFO:root:Epoch = [  1/200] Iter = [   0/ 125] Loss = 0.1073 Avg Loss = 0.1073\n",
      "INFO:root:Epoch = [  1/200] Iter = [  20/ 125] Loss = 0.1362 Avg Loss = 0.1099\n",
      "INFO:root:Epoch = [  1/200] Iter = [  40/ 125] Loss = 0.1192 Avg Loss = 0.1117\n",
      "INFO:root:Epoch = [  1/200] Iter = [  60/ 125] Loss = 0.1242 Avg Loss = 0.1142\n",
      "INFO:root:Epoch = [  1/200] Iter = [  80/ 125] Loss = 0.1283 Avg Loss = 0.1121\n",
      "INFO:root:Epoch = [  1/200] Iter = [ 100/ 125] Loss = 0.0943 Avg Loss = 0.11\n",
      "INFO:root:Epoch = [  1/200] Iter = [ 120/ 125] Loss = 0.1087 Avg Loss = 0.1109\n",
      "INFO:root:Epoch = [  2/200] Iter = [   0/ 125] Loss = 0.1953 Avg Loss = 0.1953\n",
      "INFO:root:Epoch = [  2/200] Iter = [  20/ 125] Loss = 0.1434 Avg Loss = 0.1789\n",
      "INFO:root:Epoch = [  2/200] Iter = [  40/ 125] Loss = 0.1088 Avg Loss = 0.1674\n",
      "INFO:root:Epoch = [  2/200] Iter = [  60/ 125] Loss = 0.1304 Avg Loss = 0.1568\n",
      "INFO:root:Epoch = [  2/200] Iter = [  80/ 125] Loss = 0.09432 Avg Loss = 0.1475\n",
      "INFO:root:Epoch = [  2/200] Iter = [ 100/ 125] Loss = 0.04794 Avg Loss = 0.1411\n",
      "INFO:root:Epoch = [  2/200] Iter = [ 120/ 125] Loss = 0.1234 Avg Loss = 0.1377\n",
      "INFO:root:Epoch = [  3/200] Iter = [   0/ 125] Loss = 0.09109 Avg Loss = 0.09109\n",
      "INFO:root:Epoch = [  3/200] Iter = [  20/ 125] Loss = 0.1086 Avg Loss = 0.09387\n",
      "INFO:root:Epoch = [  3/200] Iter = [  40/ 125] Loss = 0.09712 Avg Loss = 0.0956\n",
      "INFO:root:Epoch = [  3/200] Iter = [  60/ 125] Loss = 0.09497 Avg Loss = 0.09838\n",
      "INFO:root:Epoch = [  3/200] Iter = [  80/ 125] Loss = 0.1289 Avg Loss = 0.1018\n",
      "INFO:root:Epoch = [  3/200] Iter = [ 100/ 125] Loss = 0.1525 Avg Loss = 0.1042\n",
      "INFO:root:Epoch = [  3/200] Iter = [ 120/ 125] Loss = 0.09783 Avg Loss = 0.107\n",
      "INFO:root:Epoch = [  4/200] Iter = [   0/ 125] Loss = 0.1566 Avg Loss = 0.1566\n",
      "INFO:root:Epoch = [  4/200] Iter = [  20/ 125] Loss = 0.1695 Avg Loss = 0.1485\n",
      "INFO:root:Epoch = [  4/200] Iter = [  40/ 125] Loss = 0.09145 Avg Loss = 0.1423\n",
      "INFO:root:Epoch = [  4/200] Iter = [  60/ 125] Loss = 0.1521 Avg Loss = 0.1377\n",
      "INFO:root:Epoch = [  4/200] Iter = [  80/ 125] Loss = 0.1338 Avg Loss = 0.1325\n",
      "INFO:root:Epoch = [  4/200] Iter = [ 100/ 125] Loss = 0.09061 Avg Loss = 0.1291\n",
      "INFO:root:Epoch = [  4/200] Iter = [ 120/ 125] Loss = 0.07147 Avg Loss = 0.1241\n",
      "INFO:root:Epoch = [  5/200] Iter = [   0/ 125] Loss = 0.09993 Avg Loss = 0.09993\n",
      "INFO:root:Epoch = [  5/200] Iter = [  20/ 125] Loss = 0.1294 Avg Loss = 0.1017\n",
      "INFO:root:Epoch = [  5/200] Iter = [  40/ 125] Loss = 0.1115 Avg Loss = 0.1017\n",
      "INFO:root:Epoch = [  5/200] Iter = [  60/ 125] Loss = 0.1292 Avg Loss = 0.1058\n",
      "INFO:root:Epoch = [  5/200] Iter = [  80/ 125] Loss = 0.07213 Avg Loss = 0.1059\n",
      "INFO:root:Epoch = [  5/200] Iter = [ 100/ 125] Loss = 0.09699 Avg Loss = 0.1076\n",
      "INFO:root:Epoch = [  5/200] Iter = [ 120/ 125] Loss = 0.09291 Avg Loss = 0.1095\n",
      "INFO:root:Epoch = [  6/200] Iter = [   0/ 125] Loss = 0.09931 Avg Loss = 0.09931\n",
      "INFO:root:Epoch = [  6/200] Iter = [  20/ 125] Loss = 0.1635 Avg Loss = 0.1012\n",
      "INFO:root:Epoch = [  6/200] Iter = [  40/ 125] Loss = 0.08938 Avg Loss = 0.1017\n",
      "INFO:root:Epoch = [  6/200] Iter = [  60/ 125] Loss = 0.1408 Avg Loss = 0.1029\n",
      "INFO:root:Epoch = [  6/200] Iter = [  80/ 125] Loss = 0.156 Avg Loss = 0.1044\n",
      "INFO:root:Epoch = [  6/200] Iter = [ 100/ 125] Loss = 0.1496 Avg Loss = 0.1078\n",
      "INFO:root:Epoch = [  6/200] Iter = [ 120/ 125] Loss = 0.08309 Avg Loss = 0.1098\n",
      "INFO:root:Epoch = [  7/200] Iter = [   0/ 125] Loss = 0.1295 Avg Loss = 0.1295\n",
      "INFO:root:Epoch = [  7/200] Iter = [  20/ 125] Loss = 0.08309 Avg Loss = 0.1261\n",
      "INFO:root:Epoch = [  7/200] Iter = [  40/ 125] Loss = 0.1673 Avg Loss = 0.1233\n",
      "INFO:root:Epoch = [  7/200] Iter = [  60/ 125] Loss = 0.1279 Avg Loss = 0.123\n",
      "INFO:root:Epoch = [  7/200] Iter = [  80/ 125] Loss = 0.08677 Avg Loss = 0.1184\n",
      "INFO:root:Epoch = [  7/200] Iter = [ 100/ 125] Loss = 0.07973 Avg Loss = 0.1177\n",
      "INFO:root:Epoch = [  7/200] Iter = [ 120/ 125] Loss = 0.04585 Avg Loss = 0.116\n",
      "INFO:root:Epoch = [  8/200] Iter = [   0/ 125] Loss = 0.09472 Avg Loss = 0.09472\n",
      "INFO:root:Epoch = [  8/200] Iter = [  20/ 125] Loss = 0.1256 Avg Loss = 0.09704\n",
      "INFO:root:Epoch = [  8/200] Iter = [  40/ 125] Loss = 0.05684 Avg Loss = 0.0981\n",
      "INFO:root:Epoch = [  8/200] Iter = [  60/ 125] Loss = 0.1183 Avg Loss = 0.1021\n",
      "INFO:root:Epoch = [  8/200] Iter = [  80/ 125] Loss = 0.06864 Avg Loss = 0.1034\n",
      "INFO:root:Epoch = [  8/200] Iter = [ 100/ 125] Loss = 0.1722 Avg Loss = 0.1054\n",
      "INFO:root:Epoch = [  8/200] Iter = [ 120/ 125] Loss = 0.0911 Avg Loss = 0.1071\n",
      "INFO:root:Epoch = [  9/200] Iter = [   0/ 125] Loss = 0.09561 Avg Loss = 0.09561\n",
      "INFO:root:Epoch = [  9/200] Iter = [  20/ 125] Loss = 0.09383 Avg Loss = 0.09715\n",
      "INFO:root:Epoch = [  9/200] Iter = [  40/ 125] Loss = 0.1198 Avg Loss = 0.1013\n",
      "INFO:root:Epoch = [  9/200] Iter = [  60/ 125] Loss = 0.06618 Avg Loss = 0.1024\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(params.epoch):\n",
    "    single_MoDL.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        input,target,mask = data\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        mask = mask.to(device)\n",
    "        im_out = single_MoDL(input.float(),mask=mask)\n",
    "        loss = criterion(im_out,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        if iter % 20 == 0:\n",
    "            logging.info(\n",
    "                f'Epoch = [{epoch:3d}/{params.epoch:3d}] '\n",
    "                f'Iter = [{iter:4d}/{len(train_loader):4d}] '\n",
    "                f'Loss = {loss.item():.4g} Avg Loss = {avg_loss:.4g}'\n",
    "            )\n",
    "    #Saving the model\n",
    "    exp_dir = \"checkpoints/\"\n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'params': params,\n",
    "            'model': single_MoDL.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'exp_dir': exp_dir\n",
    "        },\n",
    "        f=os.path.join(exp_dir, 'model_%d.pt'%(epoch))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
